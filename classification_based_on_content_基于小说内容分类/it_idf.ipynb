{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#coding:utf-8 \n",
    "\n",
    "import jieba\n",
    "import sys\n",
    "\n",
    "from sklearn import feature_extraction\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import jieba\n",
    "import jieba.posseg as pseg\n",
    "from sklearn import feature_extraction\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 读取数据\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "catoes = ['仙侠', '体育', '军事', '历史', '奇幻', '悬疑', '武侠', '游戏', '玄幻', '现实', '科幻', '诸天无限', '轻小说', '都市']\n",
    "smallcatoes = ['仙侠', '体育', '军事', '历史']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting\n",
      "done for  《仙草供应商》.txt  in  仙侠\n",
      "done for  《凡人修仙传》.txt  in  仙侠\n",
      "done for  《剑徒之路》.txt  in  仙侠\n",
      "done for  《大奉打更人》.txt  in  仙侠\n",
      "done for  《大魏读书人》.txt  in  仙侠\n",
      "done for  《妖女哪里逃》.txt  in  仙侠\n",
      "done for  《我师兄实在太稳健了》.txt  in  仙侠\n",
      "done for  《星辰变》.txt  in  仙侠\n",
      "done for  《最强反套路系统》.txt  in  仙侠\n",
      "done for  《玄清卫》.txt  in  仙侠\n",
      "done for  《神话：天罡地煞》.txt  in  仙侠\n",
      "done for  《许仙不是剑仙》.txt  in  仙侠\n",
      "done for  《诡异修仙世界》.txt  in  仙侠\n",
      "done for  《道君》.txt  in  仙侠\n",
      "done for  《问道红尘》.txt  in  仙侠\n",
      "done for  《问道红尘》.txt  in  仙侠\n",
      "starting\n",
      "done for  《NBA王朝狙击手》.txt  in  体育\n",
      "done for  《NBA绝对防洪坝》.txt  in  体育\n",
      "done for  《你当像勇者翻过群山》.txt  in  体育\n",
      "done for  《冠军之心》.txt  in  体育\n",
      "done for  《我不想当老大》.txt  in  体育\n",
      "done for  《我就是能进球》.txt  in  体育\n",
      "done for  《我投篮实在太准了》.txt  in  体育\n",
      "done for  《我是卡卡卡卡卡》.txt  in  体育\n",
      "done for  《我真的是个内线》.txt  in  体育\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32mD:\\tempFiles/ipykernel_19036/484510426.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     18\u001b[0m                         \u001b[0mline\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"\\n\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m                         \u001b[0mline\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\" \"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m                         \u001b[0mcontent\u001b[0m\u001b[1;33m+=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m                     \u001b[0mcontent\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcontent\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mline\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"done for \"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\" in \"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcato\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import os\n",
    "upath = \"D:/Projectach/信息检索_网络小说检索分类/小说\" #文件夹目录\n",
    "contentIncatoes = []\n",
    "for cato in smallcatoes:\n",
    "    path = upath+\"/\"+cato\n",
    "    files= os.listdir(path) #得到文件夹下的所有文件名称\n",
    "    cInAcato = \"\"\n",
    "    print(\"starting\")\n",
    "    for file in files: #遍历文件夹\n",
    "         if not os.path.isdir(file): #判断是否是文件夹，不是文件夹才打开\n",
    "            fn = (path+\"/\"+file) #打开文件\n",
    "            with open(fn, 'r',encoding='utf-8') as f:\n",
    "                content = \"\"\n",
    "                for line in f: #遍历文件，一行行遍历，读取文本\n",
    "                    if line ==\"\\n\":\n",
    "                        continue\n",
    "                    else:\n",
    "                        line = line.replace(\"\\n\",\"\")\n",
    "                        line = line.replace(\" \",\"\")\n",
    "                        content+=(line)\n",
    "                    content = content + line \n",
    "                print(\"done for \",file,\" in \",cato)\n",
    "                cInAcato+=(content) #储存一本小说\n",
    "    print(\"done for \",file,\" in \",cato)#储存一类小说\n",
    "    contentIncatoes.append(cInAcato)\n",
    "\n",
    "print(len(contentIncatoes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取停用词列表\n",
    "def get_stopword_list(file):\n",
    "    with open(file, 'r', encoding='utf-8') as f:    # \n",
    "        stopword_list = [word.strip('\\n') for word in f.readlines()]\n",
    "    return stopword_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = []\n",
    "for cont in contentIncatoes:\n",
    "    stopword_list = get_stopword_list(\"D:/Projectach/信息检索_网络小说检索分类/停用词.txt\")\n",
    "    #对文档进行分词处理，采用默认模式\n",
    "    word_list = jieba.cut(cont,cut_all=True)\n",
    "    seg_list = []\n",
    "    for word in word_list:\n",
    "        if word not in stopword_list:\n",
    "            seg_list.append(word)\n",
    "    corpus.append(\" \".join(seg_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(corpus[0][2300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()    \n",
    "transformer = TfidfTransformer()\n",
    "tfidf = transformer.fit_transform(vectorizer.fit_transform(corpus))\n",
    "word = vectorizer.get_feature_names() #所有文本的关键字\n",
    "weight = tfidf.toarray()              #对应的tfidf矩阵"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将各个阶段的tf idf值、关键词等组合成一个字典\n",
    "score_dict = {}\n",
    "for i in range(len(corpus)):\n",
    "    scores = weight[i]\n",
    "    score_dict[smallcatoes[i]] = {key:value for (key,value) in zip(scores,word)}\n",
    "    # score_dict['0'] 这里的0表示的第几阶段"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "st = 3\n",
    "topk = 30\n",
    "top_30 = sorted(score_dict[smallcatoes[st]].keys(),reverse=True)[0:topk]\n",
    "for i in range(topk):\n",
    "    if i == 50:\n",
    "        print(\"______\")\n",
    "#     print(score_dict[smallcatoes[st]][top_30[i]])\n",
    "    print(score_dict[smallcatoes[st]][top_30[i]] + \":\" + str(top_30[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# corpus = []\n",
    "# with open(\"1.txt\", 'r',encoding='utf-8') as f:\n",
    "#     for line in f:\n",
    "#         corpus.append(\" \".join(jieba.cut(line.split(',')[0], cut_all=False)))\n",
    "    \n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf = vectorizer.fit_transform(corpus)\n",
    "print (tfidf.shape)\n",
    "\n",
    "words = vectorizer.get_feature_names()\n",
    "for i in range(len(corpus)):\n",
    "    print ('----Document %d----' % (i))\n",
    "    for j in range(len(words)):\n",
    "        if tfidf[i,j] > 1e-5:\n",
    "              print (words[j], tfidf[i,j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dis = []\n",
    "for i in range(len(corpus)):\n",
    "    print ('----Document %d----' % (i))\n",
    "    di = {}\n",
    "    for j in range(len(words)):\n",
    "        di[words[j]] = tfidf[i,j]\n",
    "    dis.append(di)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(dis[0][\"你好\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = []\n",
    "with open(\"《斗破苍穹》.txt\", 'r',encoding='utf-8') as f:\n",
    "    ch1 = \"\"\n",
    "    for line in f:\n",
    "        if line ==\"\\n\":\n",
    "            continue\n",
    "        else:\n",
    "            line = line.replace(\"\\n\",\"\")\n",
    "            line = line.replace(\"    \",\"\")\n",
    "            ch1+=(line)\n",
    "corpus.append(\" \".join(jieba.cut(ch1, cut_all=False)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"《星辰变》.txt\", 'r',encoding='utf-8') as f:\n",
    "    ch2 = \"\"\n",
    "    for line in f:\n",
    "        if line ==\"\\n\":\n",
    "            continue\n",
    "        else:\n",
    "            line = line.replace(\"\\n\",\"\")\n",
    "            line = line.replace(\"    \",\"\")\n",
    "            ch2+=(line)\n",
    "corpus.append(\" \".join(jieba.cut(ch2, cut_all=False)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "88279d2366fe020547cde40dd65aa0e3aa662a6ec1f3ca12d88834876c85e1a6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
